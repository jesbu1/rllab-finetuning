# training & playing with Agent Learning Framework (Alf)
# python -m alf.bin.train --root_dir=~/tmp/img_ppo --gin_file=goaltask_img_ppo_pioneer.gin --alsologtostderr
# python -m alf.bin.play --root_dir=~/tmp/img_ppo --gin_file=goaltask_img_ppo_pioneer.gin

include 'common_ppo.gin'
import alf.networks.stable_normal_projection_network

# environment config
import alf.environments.wrappers
create_environment.env_name='SocialBot-PlayGround-v0'
PlayGround.use_image_observation=True
suite_socialbot.load.gym_env_wrappers=(@FrameStack,)
create_environment.num_parallel_environments=16
PlayGround.resized_image_size=(48, 48)
GoalTask.sparse_reward=False
Agent.observation_transformer=@image_scale_transformer

# algorithm config
actor/ActorDistributionNetwork.conv_layer_params=((16, 3, 2), (32, 3, 2))
actor/ActorDistributionNetwork.fc_layer_params=(128,)
actor/ActorDistributionNetwork.activation_fn=@tf.nn.tanh
actor/ActorDistributionNetwork.continuous_projection_net=@StableNormalProjectionNetwork
StableNormalProjectionNetwork.init_means_output_factor=1e-10
StableNormalProjectionNetwork.inverse_std_transform='softplus'
StableNormalProjectionNetwork.scale_distribution=True
StableNormalProjectionNetwork.state_dependent_std=False
estimated_entropy.assume_reparametrization=True
value/ValueNetwork.conv_layer_params=((16, 3, 2), (32, 3, 2))
value/ValueNetwork.fc_layer_params=(128,)
value/ValueNetwork.activation_fn=@tf.nn.tanh
Agent.enforce_entropy_target=True

# debug
PPOLoss.check_numerics=True
estimated_entropy.check_numerics=True
TrainerConfig.summarize_action_distributions=True

# training config
TrainerConfig.eval_interval=50
TrainerConfig.mini_batch_size=2048
TrainerConfig.num_updates_per_train_step=2
TrainerConfig.unroll_length=512
TFUniformReplayBuffer.max_length=512

